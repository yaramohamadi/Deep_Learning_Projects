{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TWR-VAE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otZ8jNABlK6c"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f77TZHfrHxfX",
        "outputId": "48ebd8f3-73a7-4fbd-9227-391923c3d9ed"
      },
      "source": [
        "pip install hazm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hazm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n",
            "\r\u001b[K     |█                               | 10kB 13.5MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 19.6MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 23.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 25.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 51kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 61kB 11.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 71kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 81kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 92kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 102kB 11.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 112kB 11.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 122kB 11.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 133kB 11.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 143kB 11.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 153kB 11.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 163kB 11.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 174kB 11.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 184kB 11.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 194kB 11.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 204kB 11.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 215kB 11.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 225kB 11.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 235kB 11.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 245kB 11.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 256kB 11.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 266kB 11.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 276kB 11.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 286kB 11.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 296kB 11.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 307kB 11.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 11.7MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 20.3MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 35.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp37-none-any.whl size=1394469 sha256=f2006dad05d0ccd6e235b76465554bf3c63bac3e8164d492930d641ae4917408\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154231 sha256=9616a793fcbf7b3846a2145709215147832a5acc52d69a9783f691c4ad62561b\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST8BoKdZ1bGi",
        "outputId": "596c8010-0cec-4605-c851-3c4b51451f60"
      },
      "source": [
        "!git clone https://github.com/ruizheliUOA/TWR-VAE.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TWR-VAE'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
            "remote: Total 92 (delta 25), reused 61 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (92/92), done.\n",
            "Checking out files: 100% (41/41), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icQi1bvSlUXW"
      },
      "source": [
        "## Training on YAHOO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAMEKxnS10H2",
        "outputId": "0a536b9c-5e94-462b-d83b-c574cfb51b28"
      },
      "source": [
        "%cd /content/drive/MyDrive/TWAR-VAE/TWR-VAE/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/TWAR-VAE/TWR-VAE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka2LwI955yjq",
        "outputId": "1277c611-6b73-443f-fb17-1a5f4ca2233e"
      },
      "source": [
        "!python /content/drive/MyDrive/TWAR-VAE/TWR-VAE/lang_model/main.py -dt yahoo --z_type normal "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=64, dataset='yahoo', dropout=0.5, embedding_size=512, epochs=1000, gpu_id='0', hidden_size=256, layers=1, load=False, lr=0.0001, min_word_count=1, model_dir='', no_cuda=False, partial=False, partial_type='last75', rnn_type='lstm', save=True, setting='standard', z_type='normal', zdim=32)\n",
            "base_path= ./lang_model\n",
            "start to load Corpus data\n",
            "start to build dictionary\n",
            "start to make one-hot vectors\n",
            "start to load Corpus data\n",
            "start to build dictionary\n",
            "start to make one-hot vectors\n",
            "start to load Corpus data\n",
            "start to build dictionary\n",
            "start to make one-hot vectors\n",
            "voca_dim=19729\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Encoder(\n",
            "  (embedding): Embedding(19729, 512, padding_idx=0)\n",
            "  (rnn): LSTM(512, 256, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (linear_mu): Linear(in_features=256, out_features=32, bias=True)\n",
            "  (linear_var): Linear(in_features=256, out_features=32, bias=True)\n",
            ")\n",
            "Decoder(\n",
            "  (embedding): Embedding(19729, 512, padding_idx=0)\n",
            "  (rnn): LSTM(544, 256, dropout=0.5)\n",
            "  (z2h_c): Linear(in_features=32, out_features=256, bias=False)\n",
            "  (out): Linear(in_features=256, out_features=19729, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "--------------------------\n",
            "Time: 2m 27s| Train 0: recon_loss=476.4785, kl_loss=0.4116, nll_loss=476.8901, nll_loss_perword=5.8947, ppl=363.0999, acc=0.1194\n",
            "Eval: recon_loss:450.7572, kl_loss:1.0225, nll_loss:451.7798, nll_loss_perword=5.5610, ppl:260.0942, mi:2.3031\n",
            "Eval: recon_loss:450.6659, kl_loss:1.0203, nll_loss:451.6862, nll_loss_perword=5.5732, ppl:263.2864, mi:2.2366\n",
            "--------------------------\n",
            "Time: 2m 30s| Train 1: recon_loss=439.3767, kl_loss=0.3446, nll_loss=439.7213, nll_loss_perword=5.4352, ppl=229.3494, acc=0.1564\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 2: recon_loss=425.1450, kl_loss=0.4253, nll_loss=425.5702, nll_loss_perword=5.2603, ppl=192.5451, acc=0.1705\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 3: recon_loss=415.2207, kl_loss=0.5330, nll_loss=415.7537, nll_loss_perword=5.1390, ppl=170.5437, acc=0.1817\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 4: recon_loss=407.2604, kl_loss=0.6550, nll_loss=407.9154, nll_loss_perword=5.0421, ppl=154.7955, acc=0.1905\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 5: recon_loss=400.7645, kl_loss=0.7557, nll_loss=401.5202, nll_loss_perword=4.9631, ppl=143.0302, acc=0.1973\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 6: recon_loss=395.3222, kl_loss=0.8282, nll_loss=396.1504, nll_loss_perword=4.8967, ppl=133.8450, acc=0.2027\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 7: recon_loss=390.6375, kl_loss=0.8836, nll_loss=391.5211, nll_loss_perword=4.8395, ppl=126.4012, acc=0.2074\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 8: recon_loss=386.4220, kl_loss=0.9381, nll_loss=387.3601, nll_loss_perword=4.7880, ppl=120.0644, acc=0.2117\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 9: recon_loss=382.4936, kl_loss=0.9859, nll_loss=383.4795, nll_loss_perword=4.7401, ppl=114.4411, acc=0.2157\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 10: recon_loss=378.9429, kl_loss=1.0266, nll_loss=379.9695, nll_loss_perword=4.6967, ppl=109.5822, acc=0.2192\n",
            "Eval: recon_loss:386.6266, kl_loss:1.7704, nll_loss:388.3970, nll_loss_perword=4.7809, ppl:119.2060, mi:4.0656\n",
            "Eval: recon_loss:386.7814, kl_loss:1.7671, nll_loss:388.5485, nll_loss_perword=4.7942, ppl:120.8080, mi:4.1341\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 11: recon_loss=375.5450, kl_loss=1.0689, nll_loss=376.6139, nll_loss_perword=4.6552, ppl=105.1300, acc=0.2227\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 12: recon_loss=372.3185, kl_loss=1.1110, nll_loss=373.4295, nll_loss_perword=4.6158, ppl=101.0723, acc=0.2262\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 13: recon_loss=369.3359, kl_loss=1.1489, nll_loss=370.4848, nll_loss_perword=4.5794, ppl=97.4596, acc=0.2295\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 14: recon_loss=366.4676, kl_loss=1.1872, nll_loss=367.6547, nll_loss_perword=4.5445, ppl=94.1093, acc=0.2329\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 15: recon_loss=363.6293, kl_loss=1.2269, nll_loss=364.8561, nll_loss_perword=4.5099, ppl=90.9094, acc=0.2365\n",
            "--------------------------\n",
            "Time: 2m 32s| Train 16: recon_loss=360.9388, kl_loss=1.2671, nll_loss=362.2059, nll_loss_perword=4.4771, ppl=87.9796, acc=0.2399\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 17: recon_loss=358.3161, kl_loss=1.3035, nll_loss=359.6197, nll_loss_perword=4.4451, ppl=85.2116, acc=0.2432\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 18: recon_loss=355.7706, kl_loss=1.3396, nll_loss=357.1103, nll_loss_perword=4.4141, ppl=82.6091, acc=0.2465\n",
            "--------------------------\n",
            "Time: 2m 32s| Train 19: recon_loss=353.5466, kl_loss=1.3717, nll_loss=354.9183, nll_loss_perword=4.3870, ppl=80.4009, acc=0.2494\n",
            "--------------------------\n",
            "Time: 2m 32s| Train 20: recon_loss=351.4272, kl_loss=1.3985, nll_loss=352.8257, nll_loss_perword=4.3612, ppl=78.3480, acc=0.2521\n",
            "Eval: recon_loss:363.2949, kl_loss:2.1310, nll_loss:365.4258, nll_loss_perword=4.4981, ppl:89.8460, mi:4.1698\n",
            "Eval: recon_loss:363.4747, kl_loss:2.1254, nll_loss:365.6001, nll_loss_perword=4.5110, ppl:91.0171, mi:4.0995\n",
            "--------------------------\n",
            "Time: 2m 32s| Train 21: recon_loss=349.4951, kl_loss=1.4240, nll_loss=350.9191, nll_loss_perword=4.3376, ppl=76.5231, acc=0.2544\n",
            "--------------------------\n",
            "Time: 2m 32s| Train 22: recon_loss=347.6114, kl_loss=1.4457, nll_loss=349.0571, nll_loss_perword=4.3146, ppl=74.7820, acc=0.2568\n",
            "--------------------------\n",
            "Time: 2m 32s| Train 23: recon_loss=345.8570, kl_loss=1.4653, nll_loss=347.3224, nll_loss_perword=4.2931, ppl=73.1956, acc=0.2590\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 24: recon_loss=344.1681, kl_loss=1.4834, nll_loss=345.6516, nll_loss_perword=4.2725, ppl=71.6994, acc=0.2611\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 25: recon_loss=342.6281, kl_loss=1.5007, nll_loss=344.1289, nll_loss_perword=4.2537, ppl=70.3625, acc=0.2629\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 26: recon_loss=341.0539, kl_loss=1.5186, nll_loss=342.5725, nll_loss_perword=4.2344, ppl=69.0219, acc=0.2650\n",
            "--------------------------\n",
            "Time: 2m 31s| Train 27: recon_loss=339.6303, kl_loss=1.5350, nll_loss=341.1653, nll_loss_perword=4.2170, ppl=67.8317, acc=0.2667\n",
            "--------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj4inBxtMCav"
      },
      "source": [
        "## Create DIGIKALA Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zw0CU-7IEBE9",
        "outputId": "93eb3765-c476-44bd-c65a-281f8489be03"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/TWAR-VAE/TWR-VAE/digikala.xlsx\")\n",
        "\n",
        "df['comment'].to_numpy()[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['واقعا عالیه. من که ازش خیلی راضیم',\n",
              "       'سلام، قبل اینکه نظرم رو بگم میخواستم به یک موضوع مهم اشاره کنم که نظراتی که ما برای کالاها ثبت میکنیم خیلی مهم هستن، چون بسیاری از مردم عزیز با استناد به این نظرات یک کالا رو خریداری میکنن. پس بهتره ک نظر غیر کارشناسی و الکی ندیم.\\r\\n\\r\\nمن ۲سال این پاور بانک رو دارم، برای ۲نفر اشنا هم خریدم، پیشنهاد ویژه بود خریدم، واقعا از هر نظر عالیه، بعد گذشت ۲سال هنوزم ۵بار ایفون se رو شارژ میکنه، در خصوص زمان شارژ گفته بودن دوستان، اگر با کابل خود پاور شارژ کنید حدود ۶ساعت فول میشه، اما اگر هر کابل دیگه ای وصل کنید ۱۲تا۱۸ ساعت طول میکشه.\\r\\nهمزمان با این پاور یک adata 10000 هم برای همسرم خریدم ، اونم خوب درومد ولی حدود ۳۰\\u200e٪ افت داشته و کمتر بازدهی شارژ داره.\\r\\n\\r\\nطبق تجربه من و اطرافیانم ک شیائومی داشتن، اگر کالایی ک میخرید اصل باشه(فیک زیاد داره) واقعا شیائومی حرف اول و میزنه، \\r\\nanker هم برند خیلی خوب ولی گرونه، هفته پیش از دیجی خریدم a1214 اونم واقعا سبک و عالیه و بازدهی بالایی داره.(۲خروجی هوشمند داره،مجموع ۳امپر میده،حداکثر یک خروجی ۲.۴ میده)\\r\\n\\r\\nببخشید طولانی شد.',\n",
              "       'گیره های فلزی خیلی سخت تا میشوند و لذا حوله را خیلی سخت می توان در آورد \\r\\n\\r\\nمهسان یه مدل دیگه داره که پلاستیکی هست و خیلی ساده و راحت خم می شود\\r\\n\\r\\nبه نظرم تولید این طرح را باید متوقف کنن چون گیره های آهنی خیلی سفت هستن و وقتی می خواستم خمشون کنم پایه پلاستیکیش شکست و بلا استفاده شد',\n",
              "       'همه چیز در رابطه با ظاهر این گوشی بسیار خوب است. بدنه یکپارچه فلزی-پلاستیکی و صفحه نمایش با کیفیت حسی ارزنده را منتقل میکند. در مقایسه با قیمت از بسیاری از گوشی های همرده پرتوان تر است و امکانات زیادی را در اختیار کاربر میگذارد. تنها مورد قدرت پردازنده گرافیکی و CPU گوشی است که با توجه به رزولوشن بالای صفحه نمایش مقداری در اجرای بازی های سنگین کم می آورد. همچنین حجم زیادی از رم محدود 2 گیگی این گوشی توسط سیستم اشغال میشود و در بهترین مواقع تنها نیمی از آن قابل اسفاده برنامه های دیگر است. \\r\\nمورد دیگر کیفیت پایین دوربین و کیفیت پایینتر اسپیکر است. تصاویر هر دو دوربین بسیار پر نویز  و تار هستند. تنها با استفاده از فلش و یا در نور روز تصاویر بهتری میتوان ثبت کرد. صدایی که از اسپیکر خارج میشود خیلی بیکیفیت و دارای اکو و نویز بالاست. اما کیفیت صدای خروجی هدفون بسیار خوب است.\\r\\nباتری در کارهای متداول سبک تا معمولی کارایی خوبی دارد، اما هنگام اجرای برنامه های سنگین خیلی زود خالی میشود. زمان شارژ شدن باتری نیز نسبتا طولانی است. (حدود 3 ساعت)\\r\\nدر کل این گوشی را به کسانی که استفاده سنگین مولتی تسکینگ و گیم دارند پیشنهاد نمیکنم. اما اگر کسی هستید که وبگردی، تماشای ویدئو و استفاده از برنامه های  سبک بخش اعظم استفاده شما از گوشی موبایل هستند، دوربین و اسپیکر برایتان از نان شب واجب تر نیست و به ظاهری زیبا و بدنه با کیفیت اهمیت میدهید در خرید آن شک نکنید.',\n",
              "       'اگر ظرفیتش براتون کافیه حتما بخرید.\\r\\nیه شارژر 5 ولت 2 آمپر براش تهیه کنید تا سریعتر شارژ شه. خود برند شیائومی شارژر های خوبی داره.\\r\\nظرافتش خوبه و وزن و ضخامت کمی داره و مثل یه موبایل راحت حمل میشه. البته پیشنهاد من ظرفیت های بالاتر هست. چون این مدل بسته به ظرفیت باتری گوشیتون صرفا به اندازه یک الی 2 بار شارژ کردن، انرژی ذخیره میکنه.',\n",
              "       'سلام دوستان،،\\r\\nمنم مثه بعضی از دوستان قبل از خرید کلی تحقیق در مورد این لپ تاپ عالی انجام دادم و به این نتیجه رسیدم که از مدل های مشابه همین شرکت و شرکت های دیگه واقعا بهتره،، در طول تحقیق هم به اون ویدیو هایی که بعضی دوستامون اشاره کردن هم برخوردم که از براق بودن صفحه نمایش و لق بودن لولاهای لپ تاپ موقع استفاده از تاچ اسکرین ایراد گرفته بودن، منم خیلی ترسیدم از این بابت ولی وقتی که تصمیم گرفتم که انتخابم رو نهایی کنم و بدستم رسید لپ تاپ دیدم واقعا تمام اون گفته ها اقراری بیش نبوده و اصلا خبری از لق زدن صفحه نیست موقع تاچ مگر اینکه بخوای محکم بکوبی انگشتتو که اونوقت تو هر لپ تاپی مسئله لق زدن هست، ولی از این بابت هیچ مشکلی نیست و کاملا محکمه.\\r\\nدر مورد بازتاب بیش از حد صفحه نمایش هم همینطور فقط اقرار شده، اگر بخوای تو محیط بسته قرار داشته باشی با یه پروژکتور  رو بروی صفحه بازهم این مسئله واسه همه لپ تاپ ها هست، و اینکه اگه حتی با یه لپ تاپی که صفحه مات داشته باشه بخوای تو فضای آزاد زیر نور شدید خورشید کار کنی محتوای صفحه ی اون رو هم نمیبینی (گفتم اینارو چون لپ تاپ قبلیم صفحش مات بوده و تجربه کردم)  ولی تو شرایط عادی و استفاده روزمره به مشکلی باهاش برنمیخوری.\\r\\nامیدوارم تونسته باشم تو رفع بعضی شبهه ها واسه بعضی از دوستامون کمک کرده باشم.',\n",
              "       'من چند سالی هست که این اسپیکرو خریدم و واقعا حرف نداره \\r\\nبا برندای دیگه مثل شیائومی و جی بی ال و مارکای دیگه مقایسه کردم یه سرو گردن از همه لحاظ بالاتره اصلا تو خریدش شک نکنید',\n",
              "       'بوی تند ولی خوشبو داره.ماندگاریش خوبه و هر چقدر میگذره خوشبوتر میشه..',\n",
              "       'متاسفانه عمر مفید این ماشین کم هست و بعد از دو سال دیگه با باطری کار نمیکنه و گیر میکنه به نحوی که هیچیک از دکمه ها کار نمیکنه تا چند ساعت در این حالت میمونه دوباره با اتصال به برق کار میکنه',\n",
              "       'خوب بودممنون'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWVRxZkcEcs_"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Create dataset\n",
        "X_train, X_test = train_test_split(df['comment'].to_numpy(), test_size=0.2, random_state=42)\n",
        "X_train, X_val = train_test_split(X_train, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "# Save dataset in text format\n",
        "digikala_directory = '/content/drive/MyDrive/TWAR-VAE/TWR-VAE/lang_model/dataset/digikala/'\n",
        "\n",
        "if not os.path.exists(digikala_directory):\n",
        "    os.makedirs(digikala_directory)\n",
        "    np.savetxt(os.path.join(digikala_directory, 'digikala_train.txt'), X_train, fmt='%s')\n",
        "    np.savetxt(os.path.join(digikala_directory, 'digikala_test.txt'), X_test, fmt='%s')\n",
        "    np.savetxt(os.path.join(digikala_directory, 'digikala_val.txt'), X_val, fmt='%s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fM27DPhMOCh"
      },
      "source": [
        "## Training on DIGIKALA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRoZ5KMOMLVo",
        "outputId": "b2228c74-ccce-4c19-d6bf-2619e657c586"
      },
      "source": [
        "%cd /content/drive/MyDrive/TWAR-VAE/TWR-VAE/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/TWAR-VAE/TWR-VAE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flcWvaoWMNQA",
        "outputId": "601cae3e-c7f5-418d-8a08-69d4103d5342"
      },
      "source": [
        "!python /content/drive/MyDrive/TWAR-VAE/TWR-VAE/lang_model/main.py -dt digikala --z_type normal --lr 1e-3 --model_dir /digikala_results/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=64, dataset='digikala', dropout=0.5, embedding_size=256, epochs=30, gpu_id='0', hidden_size=256, layers=1, load=False, lr=0.001, min_word_count=1, model_dir='/digikala_results/', no_cuda=False, partial=False, partial_type='last75', rnn_type='lstm', save=True, setting='standard', z_type='normal', zdim=32)\n",
            "base_path= ./lang_model\n",
            "start to load Corpus data\n",
            "start to build dictionary\n",
            "start to make one-hot vectors\n",
            "start to load Corpus data\n",
            "start to build dictionary\n",
            "start to make one-hot vectors\n",
            "start to load Corpus data\n",
            "start to build dictionary\n",
            "start to make one-hot vectors\n",
            "voca_dim=16166\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Encoder(\n",
            "  (embedding): Embedding(16166, 256, padding_idx=0)\n",
            "  (rnn): LSTM(256, 256, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (linear_mu): Linear(in_features=256, out_features=32, bias=True)\n",
            "  (linear_var): Linear(in_features=256, out_features=32, bias=True)\n",
            ")\n",
            "Decoder(\n",
            "  (embedding): Embedding(16166, 256, padding_idx=0)\n",
            "  (rnn): LSTM(288, 256, dropout=0.5)\n",
            "  (z2h_c): Linear(in_features=32, out_features=256, bias=False)\n",
            "  (out): Linear(in_features=256, out_features=16166, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "--------------------------\n",
            "Time: 0m 35s| Train 0: recon_loss=46.6237, kl_loss=0.2068, nll_loss=46.8305, nll_loss_perword=4.9455, ppl=140.5440, acc=0.1641\n",
            "Eval: recon_loss:41.6337, kl_loss:0.2278, nll_loss:41.8615, nll_loss_perword=4.4352, ppl:84.3657, mi:1.4968\n",
            "Eval: recon_loss:41.8673, kl_loss:0.2236, nll_loss:42.0909, nll_loss_perword=4.4277, ppl:83.7425, mi:1.5077\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 1: recon_loss=40.4445, kl_loss=0.4178, nll_loss=40.8623, nll_loss_perword=4.3152, ppl=74.8317, acc=0.2158\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 2: recon_loss=38.3348, kl_loss=0.4405, nll_loss=38.7753, nll_loss_perword=4.0948, ppl=60.0300, acc=0.2323\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 3: recon_loss=36.9186, kl_loss=0.4403, nll_loss=37.3590, nll_loss_perword=3.9453, ppl=51.6908, acc=0.2421\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 4: recon_loss=35.8266, kl_loss=0.4335, nll_loss=36.2601, nll_loss_perword=3.8292, ppl=46.0273, acc=0.2477\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 5: recon_loss=34.8951, kl_loss=0.4386, nll_loss=35.3337, nll_loss_perword=3.7314, ppl=41.7377, acc=0.2528\n",
            "Eval: recon_loss:38.2659, kl_loss:0.4147, nll_loss:38.6806, nll_loss_perword=4.0982, ppl:60.2290, mi:2.7204\n",
            "Eval: recon_loss:38.4291, kl_loss:0.4114, nll_loss:38.8406, nll_loss_perword=4.0858, ppl:59.4913, mi:2.7318\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 6: recon_loss=34.0585, kl_loss=0.4495, nll_loss=34.5080, nll_loss_perword=3.6442, ppl=38.2522, acc=0.2583\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 7: recon_loss=33.2423, kl_loss=0.4935, nll_loss=33.7358, nll_loss_perword=3.5627, ppl=35.2566, acc=0.2648\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 8: recon_loss=32.6097, kl_loss=0.4960, nll_loss=33.1058, nll_loss_perword=3.4961, ppl=32.9873, acc=0.2699\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 9: recon_loss=32.0142, kl_loss=0.5082, nll_loss=32.5224, nll_loss_perword=3.4345, ppl=31.0164, acc=0.2753\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 10: recon_loss=31.4998, kl_loss=0.5190, nll_loss=32.0189, nll_loss_perword=3.3813, ppl=29.4101, acc=0.2807\n",
            "Eval: recon_loss:38.1205, kl_loss:0.5643, nll_loss:38.6848, nll_loss_perword=4.0986, ppl:60.2557, mi:2.9970\n",
            "Eval: recon_loss:38.2674, kl_loss:0.5674, nll_loss:38.8347, nll_loss_perword=4.0852, ppl:59.4548, mi:3.0672\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 11: recon_loss=31.0481, kl_loss=0.5081, nll_loss=31.5562, nll_loss_perword=3.3325, ppl=28.0077, acc=0.2860\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 12: recon_loss=30.6054, kl_loss=0.5149, nll_loss=31.1204, nll_loss_perword=3.2865, ppl=26.7478, acc=0.2906\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 13: recon_loss=30.1800, kl_loss=0.5377, nll_loss=30.7177, nll_loss_perword=3.2439, ppl=25.6343, acc=0.2962\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 14: recon_loss=29.7009, kl_loss=0.5807, nll_loss=30.2816, nll_loss_perword=3.1979, ppl=24.4804, acc=0.3035\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 15: recon_loss=29.2404, kl_loss=0.6123, nll_loss=29.8527, nll_loss_perword=3.1526, ppl=23.3964, acc=0.3100\n",
            "Eval: recon_loss:37.7876, kl_loss:0.5943, nll_loss:38.3819, nll_loss_perword=4.0665, ppl:58.3526, mi:3.6663\n",
            "Eval: recon_loss:37.9507, kl_loss:0.5965, nll_loss:38.5472, nll_loss_perword=4.0550, ppl:57.6832, mi:3.6713\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 16: recon_loss=28.7926, kl_loss=0.6565, nll_loss=29.4491, nll_loss_perword=3.1100, ppl=22.4202, acc=0.3167\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 17: recon_loss=28.3216, kl_loss=0.7010, nll_loss=29.0225, nll_loss_perword=3.0649, ppl=21.4326, acc=0.3237\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 18: recon_loss=27.8146, kl_loss=0.7686, nll_loss=28.5832, nll_loss_perword=3.0185, ppl=20.4609, acc=0.3312\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 19: recon_loss=27.2712, kl_loss=0.8448, nll_loss=28.1160, nll_loss_perword=2.9692, ppl=19.4759, acc=0.3388\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 20: recon_loss=26.7376, kl_loss=0.9020, nll_loss=27.6395, nll_loss_perword=2.9189, ppl=18.5202, acc=0.3478\n",
            "Eval: recon_loss:36.4936, kl_loss:0.9145, nll_loss:37.4082, nll_loss_perword=3.9633, ppl:52.6328, mi:3.9996\n",
            "Eval: recon_loss:36.6390, kl_loss:0.9185, nll_loss:37.5575, nll_loss_perword=3.9509, ppl:51.9800, mi:4.0346\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 21: recon_loss=26.2564, kl_loss=0.9522, nll_loss=27.2086, nll_loss_perword=2.8734, ppl=17.6963, acc=0.3556\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 22: recon_loss=25.8070, kl_loss=0.9956, nll_loss=26.8027, nll_loss_perword=2.8305, ppl=16.9537, acc=0.3633\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 23: recon_loss=25.3376, kl_loss=1.0211, nll_loss=26.3586, nll_loss_perword=2.7836, ppl=16.1770, acc=0.3717\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 24: recon_loss=24.9229, kl_loss=1.0726, nll_loss=25.9955, nll_loss_perword=2.7452, ppl=15.5685, acc=0.3789\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 25: recon_loss=24.5124, kl_loss=1.1202, nll_loss=25.6326, nll_loss_perword=2.7069, ppl=14.9831, acc=0.3860\n",
            "Eval: recon_loss:35.2613, kl_loss:1.1558, nll_loss:36.4171, nll_loss_perword=3.8583, ppl:47.3866, mi:4.0633\n",
            "Eval: recon_loss:35.4353, kl_loss:1.1592, nll_loss:36.5944, nll_loss_perword=3.8495, ppl:46.9717, mi:4.0616\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 26: recon_loss=24.0958, kl_loss=1.1922, nll_loss=25.2880, nll_loss_perword=2.6705, ppl=14.4476, acc=0.3948\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 27: recon_loss=23.6585, kl_loss=1.2339, nll_loss=24.8924, nll_loss_perword=2.6288, ppl=13.8565, acc=0.4027\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 28: recon_loss=23.2805, kl_loss=1.2920, nll_loss=24.5725, nll_loss_perword=2.5950, ppl=13.3961, acc=0.4093\n",
            "--------------------------\n",
            "Time: 0m 36s| Train 29: recon_loss=22.9222, kl_loss=1.3338, nll_loss=24.2560, nll_loss_perword=2.5615, ppl=12.9559, acc=0.4146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa1eYeTb-4LG"
      },
      "source": [
        "## Testing on DIGIKALA\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gz3mYKGMNKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b72c27-2096-48eb-c412-c86fe68b8ca5"
      },
      "source": [
        "!python /content/drive/MyDrive/TWAR-VAE/TWR-VAE/lang_model/main.py -dt digikala --z_type normal --lr 1e-3 --model_dir ./lang_model/digikala_model_save/ --load --epochs 31"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=64, dataset='digikala', dropout=0.5, embedding_size=256, epochs=31, gpu_id='0', hidden_size=256, layers=1, load=True, lr=0.001, min_word_count=1, model_dir='./lang_model/digikala_model_save/', no_cuda=False, partial=False, partial_type='last75', rnn_type='lstm', save=True, setting='standard', z_type='normal', zdim=32)\n",
            "base_path= ./lang_model\n",
            "start to load Corpus data\n",
            "start to build dictionary\n",
            "start to make one-hot vectors\n",
            "start to load Corpus data\n",
            "start to build dictionary\n",
            "start to make one-hot vectors\n",
            "start to load Corpus data\n",
            "start to build dictionary\n",
            "start to make one-hot vectors\n",
            "voca_dim=16166\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Encoder(\n",
            "  (embedding): Embedding(16166, 256, padding_idx=0)\n",
            "  (rnn): LSTM(256, 256, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (linear_mu): Linear(in_features=256, out_features=32, bias=True)\n",
            "  (linear_var): Linear(in_features=256, out_features=32, bias=True)\n",
            ")\n",
            "Decoder(\n",
            "  (embedding): Embedding(16166, 256, padding_idx=0)\n",
            "  (rnn): LSTM(288, 256, dropout=0.5)\n",
            "  (z2h_c): Linear(in_features=32, out_features=256, bias=False)\n",
            "  (out): Linear(in_features=256, out_features=16166, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "Eval: recon_loss:35.2613, kl_loss:1.1558, nll_loss:36.4171, nll_loss_perword=3.8583, ppl:47.3866, mi:4.0601\n",
            "Eval: recon_loss:35.4353, kl_loss:1.1592, nll_loss:36.5944, nll_loss_perword=3.8495, ppl:46.9717, mi:4.0383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzhoaeHBMNCj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8hZgCTVMMfS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}