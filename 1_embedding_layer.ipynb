{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZ81GRJUl2zb"
   },
   "source": [
    "## Import Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vA28RSKniSY7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from numpy.linalg import norm\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yxiDISkEgsJ"
   },
   "source": [
    "## Override Keras Embedding Class\n",
    "\n",
    "similarity is measured by Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1N-uvH4IiYZo"
   },
   "outputs": [],
   "source": [
    "class Embedding2(Embedding):\n",
    "  def measure_similarity(self, a, b):\n",
    "    return (a @ b.T) / (norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YR4PGJ8CEnNn"
   },
   "source": [
    "## Classification model to train on text data\n",
    "\n",
    "this code is partly copy pasted from the internet and is only used so that my embedding layer is trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0g0WXlG5_ph",
    "outputId": "53a7dfc3-c987-4f69-8080-4b8bd6e4b3b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16, 3], [32, 46], [6, 35], [25, 46], [2], [5], [43, 35], [3, 32], [43, 46], [9, 18, 3, 7]]\n",
      "[[16  3  0  0]\n",
      " [32 46  0  0]\n",
      " [ 6 35  0  0]\n",
      " [25 46  0  0]\n",
      " [ 2  0  0  0]\n",
      " [ 5  0  0  0]\n",
      " [43 35  0  0]\n",
      " [ 3 32  0  0]\n",
      " [43 46  0  0]\n",
      " [ 9 18  3  7]]\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embed (Embedding2)           (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f85281e55f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding2(vocab_size, 8, input_length=max_length, name='embed'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xj2JTiXCFs0x"
   },
   "source": [
    "## Get output of Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_nnRDTo6R6X"
   },
   "outputs": [],
   "source": [
    "embedding_model = Model(inputs=model.input,\n",
    "                              outputs= model.get_layer('embed').output)\n",
    "\n",
    "embedding_output = embedding_model(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFcPVUagBHWw"
   },
   "source": [
    "## Measuring similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6btweiE8C1p",
    "outputId": "ca46274c-19a7-4f7d-c24a-13ee14888afa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well - Good : Strong positive correlation\n",
      "0.39971587\n",
      "Well - Work: I dont know\n",
      "-0.4904759\n",
      "Weak - Done: I dont know\n",
      "0.020694826\n",
      "Weak - Good: Strong negative correlation\n",
      "-0.5158577\n"
     ]
    }
   ],
   "source": [
    "well_done = docs[0]\n",
    "well_embedding = embedding_output[0][0]\n",
    "done_embedding = embedding_output[0][1]\n",
    "\n",
    "good_work = docs[1]\n",
    "good_embedding = embedding_output[1][0]\n",
    "work_embedding = embedding_output[1][1]\n",
    "\n",
    "weak = docs[5]\n",
    "weak_embedding = embedding_output[5][0]\n",
    "\n",
    "a = model.get_layer('embed').measure_similarity(good_embedding.numpy(), well_embedding.numpy())\n",
    "print(\"Well - Good : Strong positive correlation\")\n",
    "print(a)\n",
    "b = model.get_layer('embed').measure_similarity(well_embedding.numpy(), work_embedding.numpy())\n",
    "print(\"Well - Work: I dont know\")\n",
    "print(b)\n",
    "c = model.get_layer('embed').measure_similarity(weak_embedding.numpy(), done_embedding.numpy())\n",
    "print(\"Weak - Done: I dont know\")\n",
    "print(c)\n",
    "d = model.get_layer('embed').measure_similarity(weak_embedding.numpy(), good_embedding.numpy())\n",
    "print(\"Weak - Good: Strong negative correlation\")\n",
    "print(d)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VZ81GRJUl2zb",
    "2yxiDISkEgsJ",
    "YR4PGJ8CEnNn",
    "xj2JTiXCFs0x",
    "MFcPVUagBHWw"
   ],
   "name": "ADL_1_embedding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
